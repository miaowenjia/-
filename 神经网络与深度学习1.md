# 神经网络与深度学习1

本文是一个零基础开始学习神经网络与深度学习的文章，从环境配置、代码学习到项目实践的学习笔记。

本系列的参考书目为：
李沐等，动手学深度学习（2023年8月中/英文版）

本期内容为神经网络与深度学习概述、环境配置以及线性分类与感知机。

## 神经网络与深度学习概述

人工智能、机器学习、神经网络、深度学习之间的关系可用下图简要描述。

![](/figure/1.png)

神经网络是受到人脑神经元结构启发而设计的一种模型，用于模拟和解决复杂的模式识别问题。简单来说，神经网络的结构包含输入层、隐藏层和输出层，输入层中的多个输入在隐藏层中做多次线性、非线性的组合，再从输出层中进行输出。

![](/figure/2.png)

神经网络是深度学习的基础，深度学习的关键特征是深度神经网络，这个神经网络有多个层（深度），可用让它们学习复杂的表示和特征。深度学习的提出解决了自动提取特征的难题，解决了特征提取的局限性。

## 环境配置

### python环境配置

本文使用Python代码与PyTorch学习神经网络与深度学习。

初学者配置环境时推荐使用Anaconda来进行环境配置，首先下载并安装Anaconda，打开Anaconda Prompt，检查Anaconda是否安装成功，输入指令：

```shell
conda -V
```

若安装成功且Anaconda已经添加到环境变量中，则会输出已安装的Anaconda版本；若输出的不是已安装的Anaconda版本，或输出“不是内部指令”，则可能是环境变量中的path没有添加Anaconda，可以在网上查询相关解决方案。

安装好Anaconda后，创建一个虚拟环境“pytorch”，推荐使用3.9以上的python版本，本文使用的是3.10，输入指令：

```shell
conda create -n pytorch python=3.10
```

然后在命令框中根据指令输入Y即可。

创建好环境后可在conda命令提示框中输入下面的命令来进入虚拟环境

```shell
conda activate pytorch
```

### pytorch配置

pytorch分为CPU版本与GPU版本，CPU版本只能使用核心处理器，在后续进行图像识别或处理时速度会稍慢；GPU版本可以使用独立显卡对图像进行处理，速度较快。

作者的显卡配置为英伟达RTX 4070独立显卡，因此后续内容都是基于GPU版本进行安装（CPU版本的安装也可以在网上查询，并不影响代码实现，只是处理速度会有差别）。

使用win+R并输入cmd打开命令提示框，输入以下命令查看驱动

```shell
nvidia-smi
```

如果存在驱动则会显示如下驱动信息：

![](/figure/3.png)

若没有显示驱动信息，则可能是驱动未安装，可以到英伟达官网下载驱动。

在浏览器中输入pytorch进入pytorch官网，可以浏览到各个版本的pytorch信息，包括GPU版本的安装指令、CPU版本的安装指令。

![](/figure/4.png)

在显卡驱动信息的右上角显示CUDA Version：12.9，则对应可以安装的pytorch版本的CUDA应该<=12.9，作者在这里选择PyTorch Stable Windows Pip Python CUDA12.6版本。

使用conda命令提示框，输入以下命令进入虚拟环境

```shell
conda activate pytorch
```

注意：官网中使用的命令是pip3 install，作者在下载时如果直接使用该命令则会出现超出时间限制的情况，需要把命令中的pip3 install更改为pip install（不能换源下载，因为可能会下载到不同版本的pytorch，如果下载太慢，可以搜索教程本地安装）。输入命令：

```shell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
```

GPU版本的pytorch相对于CPU版本占用的空间较大，在安装过程中会出现2.5G左右的下载量，若没有出现，则下载的可能为CPU版本。下载结束后在虚拟环境中的命令框中依次输入以下代码来检测pytorch是否正确安装：

```shell
python
```

```python
import torch
torch.cuda.is_available()
```

如果输出为True，则说明pytorch已经完成安装，且安装的是支持GPU版本的pytorch。

### 编译器配置

在编写代码之前，需要下载Visual Studio Code和PyCharm Community两个编程软件。

#### VS Code

下载好VS Code之后，按Ctrl+Shift+X打开扩展，搜索并安装：Chinese(simplified)、Python、Jupyter、Copilot这四个扩展，然后按Ctrl+Shift+P并输入Configure Display Language，把语言切换为中文并重启VS Code。

下载好扩展包之后，需要将虚拟环境添加到VS Code中。创建一个测试文件夹（随便什么地方，可以在D盘根目录下），并点击VS Code左上角的“文件”——“打开文件夹”并打开该文件夹，再新建一个文件名为python文件，先点击右下角标红处，再选择"pytorch"虚拟环境。

![](/figure/5.png)

输入并运行代码：

```python
import torch
print(torch.cuda.is_available())
```

如果环境无误，则会输出True。

若该方法无法正常导入虚拟环境，则还可以使用另一种方法：

进入conda命令提示框中，依次输入：

```shell
conda activate pytorch
```

```shell
where python
```

则可以找到该虚拟环境在系统中的地址

![](/figure/6.png)

再次回到VS Code中，同样点击右下角，在弹出窗口中选择“输入解释器路径”，按照上方命令输出的虚拟环境地址进入，将python.exe作为解释器即可。再次输入代码检查环境是否正确：

```python
import torch
print(torch.cuda.is_available())
```

### Pycharm Community

下载并安装好pycharm之后，随便创建一个项目来对编译器进行配置。

进入项目中后，使用Ctrl+Alt+S打开设置，在搜索框中搜索Chinese进行汉化，重启pycharm。回到项目中后，再次使用Ctrl+Alt+S打开设置，点击“项目：XXX”——“Python解释器”——“添加解释器”——“添加本地解释器”——“现有”并找到上述地址中的python.exe文件即可。

![](/figure/7.png)

同样输入并运行下述代码来验证环境是否配置无误。

```python
import torch
print(torch.cuda.is_available())
```
## 线性分类与感知机

之前说过神经网络实际就是多个输入端之间进行多次线性与非线性的组合，因此先从“线性问题“入手。

### 线性回归

1. 线性模型是指目标可以表示为特征的加权和，例如：房价可能会受到房屋面积和房屋年龄的影响，可以表示为

$$
price=w_{area} \cdot area+w_{age} \cdot age + b 
$$

​	上式中的area和age都是特征，w_area和w_age成为权重，权重决定了每个特征对预测结果的影响，b是偏置。

2. 线性回归的要素

   训练集或训练数据中的特征——输入数据（一般称为x）；

   输出数据（一般称为y）；

   拟合的函数（或称为假设或模型），一般写作
   
$$
\hat{y}=h(x)
或
\hat{y}=kx+b(\hat{y}表示预测值)
$$
   
   训练的条目数：一条训练数据是由一堆输入数据和输出数据组成的，输入数据的维度为n（特征的维度）。

4. 将式特征数扩展到d维，则可写为

$$
\hat{y}=w_1x_1+w_2x_3+...+w_dx_d+b
$$
   
   也可以写成矩阵形式

$$
\hat{y}=h_\theta(\mathbf{x})=\mathbf {w}^T\mathbf {x}+b
$$
   
   上式中表示了一条训练数据中有d维特征时的预测情况，当训练数据扩展到n条时，则可写为
   
$$
\mathbf{\hat{y}}=\mathbf{h}_{\Theta}(\mathbf{X})=\mathbf {X}\mathbf {w}+b
\\
\mathbf{X}=\begin{bmatrix}
x_{_{1} }^T  \\
x_{_{2} }^T  \\
\vdots  \\
x_{_{n} }^T  \\
\end{bmatrix}
$$

5. 损失函数

   在有了模型后，还需要一种度量模型质量的方式——损失函数

$$
J(\Theta)=\frac{1}{2n}\sum_{i=1}^n(y^{(i)}-h_\theta(\mathbf{x}^{(i)}))^2
$$
   
   模型训练的目标就是找到一组合适的超平面参数**w**与b，使得损失函数的值最小，即

$$
\min_\Theta J(\Theta)
$$

   在线性回归模型中，可以得到解析解
   
$$
\cfrac{\partial J(\Theta)}{\partial \Theta}=0\\
\Theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$
   
   但在实际情况中，很少会出现可以求出解析解的情况，往往只能通过数值方法求出数值解。

6. 线性分类问题

   线性分类器可以借助特征之间的线性组合来做出分类决定，即找到一条或几条直线（或超平面）来将对象进行分类。

   输入：特征向量；

   输出：哪一类（如果是二分类问题则输出0和1，或是属于某类的概率——0~1之间的数）

   ![](figure\8.png)

   思路：构造一条直线（超平面），将各点的特征带入直线（超平面）中时，就可以通过值的正负来判断其位于直线（超平面）的上方或下方。但我们最终需要概率，结果需要位于0~1之间，因此需要对值进行一个变换：
   
$$
\theta_1x_1+\theta_2x_2+\theta_0=0\\
z=\theta_1x_1+\theta_2x_2+\theta_0\\
\hat{y}=h_\theta(\mathbf{x}^{(i)})=\cfrac{1}{1+e^{-z}}
$$
   
   经过式（1.9）中的变换，可将“点在直线上方/下方“的问题等价为”输出y位于0.5~1之间/0~0.5之间“。同样也需要给定一个损失函数
   
$$
J(\Theta)=\cfrac{1}{2n}\sum_{i=1}^n(y^{(i)}-h_\Theta(\mathbf{x}^{(i)}))^2
$$
   
   注意这里的y只能取0/1。
   在这个损失函数中，损失函数变成了非线性，无法直接求出解析解，因此只能使用数值解法。

7. 梯度下降法

   算法步骤如下：

   （1）初始化模型参数，如随机初始化；

   （2）从数据集中随机抽取小批量样本**B**且在负梯度上更新参数。

   用数学表达式描述：

$$
\theta=\theta-\cfrac{\eta}{|\mathbf{B}|}\sum_{i\in\mathbf{B}}\cfrac{\partial(y^{(i)}-h_{\theta}(\mathbf{x}^{(i)}))^2}{\part \theta}\\
\mathbf{w}=\mathbf{w}-\cfrac{\eta}{|\mathbf{B}|}\sum_{i\in\mathbf{B}}\cfrac{\partial(y^{(i)}-h_{\theta}(\mathbf{x}^{(i)}))^2}{\part \mathbf{w}}\\
b=b-\cfrac{\eta}{|\mathbf{B}|}\sum_{i\in\mathbf{B}}\cfrac{\partial(y^{(i)}-h_{\theta}(\mathbf{x}^{(i)}))^2}{\part b}\\
$$
   
   其中，学习率和样本数通常都是预先手动设定，仅需要迭代求出**w**和b即可。

   但梯度下降法也有局限性：

   批量下降法能够找到全局最优点，但收敛速度太慢，浪费学习时间；

   随机梯度下降法虽然收敛速度快，但容易陷入局部最优；

   小批量下降法每次可以使用一小批样本进行训练，收敛速度块且相比随机梯度下降法而言更容易找到全局最优点，但其对学习率也非常敏感，对不适宜的学习率也会陷入局部最优。

8. 线性回归的代码实现

   在这里选用相对简单的带有噪声的线性模型来构造数据集，任务是使用这个有噪声的有限样本数据集来恢复这个模型的参数。
   
$$
\mathbf{y}=\mathbf{X}\mathbf{w}+b+\varepsilon\\
\mathbf{w}=\begin{bmatrix}
2  &  -3.4
\end{bmatrix},b=4.2
$$

   ```python
   import random
   import torch
   ```

   ```python
   def synthetic_data(w,b,num_examples):
       '''生成y=Xw+b+噪声'''
       X = torch.normal(0, 1, (num_examples, len(w)))
       y = torch.matmul(X,w) + b
       y += torch.normal(0, 0.01, y.shape)
       return X, y.reshape((-1, 1))
   ```

   ```python
   true_w = torch.tensor([2, -3.4])
   true_b = 4.2
   features, labels = synthetic_data(true_w, true_b, 1000)
   ```

   features中的每一行都包含一个二维数据样本，labels中的每一行都包含一维标签值（一个标量）

   ```python
   features, labels # 查看随机取出的一组features和labels
   ```

   需要每次抽取小批量样本来更新模型，所以需要定义一个函数，该函数能打乱数据集中的样本并以小批量的方式来获取数据

   ```python
   def data_iter(batch_size, features, labels):
       num_examples = len(features)
       indices = list(range(num_examples))
       random.shuffle(indices)
       for i in range(0, num_examples, batch_size):
           batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])
           yield features[batch_indices], labels[batch_indices]  
           
   batch_size = 10
   
   for X,y in data_iter(batch_size, features, labels):
       print(X, '\n', y)
       break
   ```

   有了小批量样本，接下来就需要一个初始化的模型来进行训练

   ```python
   w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # requries_grade代表是否需要通过梯度来训练
   b = torch.zeros(1, requires_grad=True)
   
   def linreg(X, w, b):
       '''线性回归模型'''
       return torch.matmul(X, w) + b
   
   def squared_loss(y_hat, y):
       '''定义均方损失'''
       return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
   
   def sgd(params, lr, batch_size): # lr代表学习率
       '''对params中的参数分别进行小批量随机梯度下降'''
       with torch.no_grad():
           for param in params:
               param -= lr * param.grad / batch_size
               param.grad.zero_()
               
   lr = 0.03
   num_epochs = 3
   net = linreg
   loss = squared_loss
   ```

   有了模型、损失函数和梯度下降方法之后，就要进行参数更新，直到这些参数足以拟合我们的数据

   ```python
   for epoch in range(num_epochs):
       for X, y in data_iter(batch_size, features, labels):
           l = loss(net(X, w, b), y) # X和y的小批量损失
           # 因为l的形状是(batch_size, 1)而不是标量，因此需要将其所有元素加在一起来计算关于[w, b]的梯度
           l.sum().backward() # 求和后进行反向传播
           sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数
       with torch.no_grad():
           train_l = loss(net(features, w, b), labels)
           print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
   ```

   ```
   epoch 1, loss 0.045112
   epoch 2, loss 0.000198
   epoch 3, loss 0.000049
   ```

   再输出估计误差来观察模型拟合程度

   ```python
   print(f'w的估计误差是：{true_w - w.reshape(true_w.shape)}')
   print(f'b的估计误差是：{true_b - b}')
   ```

   ```
   w的估计误差是：tensor([ 0.0005, -0.0007], grad_fn=<SubBackward0>)
   b的估计误差是：tensor([0.0012], grad_fn=<RsubBackward1>)
   ```

9. 线性回归的简洁实现

   我们可以通过调用pytorch框架中现有的API来读取数据，可将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。

   ```python
   from torch.utils import data
   
   # 将data_arrays里面的数据按照批量大小每次（是否随机）选择出来
   def load_array(data_arrays, batch_size, is_train=True):
       '''构造一个pytorch数据迭代器'''
       dataset = data.TensorDataset(*data_arrays)
       return data.DataLoader(dataset, batch_size, shuffle=is_train) # is_train代表是否用于训练（在每轮训练中打乱数据）
   
   ```

   将数据中每批取10个放入数据迭代器中

   ```python
   batch_size = 10
   data_iter = load_array((features, labels), batch_size)
   ```

   每次取出的数据都不一样

   ```python
   next(iter(data_iter))
   ```

   ```
   [tensor([[-0.5667, -0.6789],
            [ 0.2148, -0.0394],
            [-1.0998,  0.3105],
            [ 1.7271,  1.9814],
            [-0.9196, -0.8750],
            [ 1.1136,  0.8771],
            [-0.8499, -2.1416],
            [-0.0124,  2.5012],
            [ 0.6964,  0.6722],
            [ 1.5590,  0.8128]]),
    tensor([[ 5.3773],
            [ 4.7589],
            [ 0.9514],
            [ 0.9300],
            [ 5.3260],
            [ 3.4574],
            [ 9.7711],
            [-4.3158],
            [ 3.3053],
            [ 4.5558]])]
   ```

   定义模型，首先定义一个模型变量net，该模型变量是一个Sequential类的实例，将多个层串联起来。当给定输入数据时，Sequential实例将数据输入到第一层，然后将第一层的输出作为第二层的输入，以此类推。下面代码中，我们只定义了一层网络，且该层网络使用了线性网络nn.Linear，该网络的输入参数中，第一个参数代表特征的维度，第二个参数代表输出的维度，由于features中有两个维度、labels是标量即只有一个维度，因此输入参数(2,1)。

   ```python
   from torch import nn
   # nn是神经网络的缩写
   
   net = nn.Sequential(nn.Linear(2,1))
   ```

   在使用模型net之前，还需要对模型参数初始化，如在线性回归模型中设置权重和偏置（深度学习框架通常有预定义的方法来初始化参数）。在这里我们将每个权重参数从均值为0、标准差为0.01的正态分布中进行随机取样，偏置参数初始化为0。

   ```python
   net[0].weight.data.normal_(0, 0.01)
   net[0].bias.data.fill_(0)
   ```

   然后再定义损失函数，在神经网络中，计算均方误差使用的是MSELoss类，也成为平方L_2范数，默认情况下它将返回所有样本损失的平均值。

   ```python
   loss = nn.MSELoss()
   ```

   需要进行优化的参数为权重和偏置，这两项可以使用net.parameters()来读取，且在pytorch中的optim模块中实现了很多的优化算法，这里我们还是选择使用SGD算法来进行优化。

   ```python
   trainer = torch.optim.SGD(net.parameters(), lr=0.03)
   ```

   有了训练器之后就可以开始进行训练了，通过深度学习框架的高级API来实现仅需要少量的代码，并且使用trainer.step()即可自动进行训练。

   ```python
   num_epochs = 3
   for epoch in range(num_epochs):
       for X, y in data_iter:
           # 定义损失函数
           l = loss(net(X), y)
           #反向传播
           trainer.zero_grad()
           l.backward() 
           #更新参数
           trainer.step()
       l = loss(net(features), labels)
       print(f'epoch {epoch+1}, loss {l:f}')
   ```

   ```
   epoch 1, loss 0.000314
   epoch 2, loss 0.000094
   epoch 3, loss 0.000095
   ```

   所得到的估计参数与生成数据所用到的实际参数非常接近。

   ```python
   w = net[0].weight.data
   print('w的误差:', true_w - w)
   b = net[0].bias.data
   print('b的偏差:', true_b - b)
   ```

   ```
   w的误差: tensor([[0.0004, 0.0001]])
   b的偏差: tensor([-0.0010])
   ```

   








