# 神经网络与深度学习2

在之前我们说过，神经网络其实就是多个输入通过多种的线性和非线性组合来获得的输出。若对于神经网络的结构还存有疑惑，可以打开这个网址“[从函数到神经网络【白话DeepSeek01】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1uGA3eLEeu?spm_id_from=333.788.videopod.sections&vd_source=a9e7307fd7cf6202b163a39b03746807)”，这个视频很好地讲述了神经网络每个层之间的关系以及神经元与神经元之间的关系。

这期文章我们将真正进入深度神经网络，首先需要从多层感知机开始。

【温馨提示】这一期公式稍微有亿点多，但原理和之前所说的梯度下降法大差不差，若是对梯度下降法的原理还不太清晰，可以观看视频“[【梯度下降】3D可视化讲解通俗易懂_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV18P4y1j7uH/?spm_id_from=333.1007.top_right_bar_window_history.content.click)”。

## 多层感知机

在前面的神经网络模型中只有输入层和输出层，这样的一个神经网络就是单层神经网络。

![](/figure/9.png)

$$
z_1 = w_{11} x_1 + w{12} x_2 + w{13} x_3 + w_{14} x_4 (线性输出部分)
$$

$$
o_1 = f(z_1) (在线性输出部分作用一个激活函数作为最终的输出)
$$

这样的单层神经网络有一个限制：无论你在输出层中使用什么样的激活函数，其输出都只依赖于线性输出部分。即在一个分类模型中，激活函数作用之前的非线性输出部分可能是（举个例子）：

$$
z_1^{(1)} = 1, z_2^{(1)}=10,z_3^{(1)}=2
$$

在这个时候，我们就已经可以将样本1分到类别2中了，无论我们使用什么样的激活函数，都不会改变其最终结果，例如sigmoid函数：

$$
o_1^{(1)} = \sigma (z_1^{(1)}) = \frac{1}{1+e^{-z_1^{(1)}}} \approx 0.73106
$$

$$
o_2^{(1)} = \sigma (z_2^{(1)}) = \frac{1}{1+e^{-z_2^{(1)}}} \approx 0.99995
$$

$$
o_3^{(1)} = \sigma (z_3^{(1)}) = \frac{1}{1+e^{-z_3^{(1)}}} \approx 0.88080
$$

最终还是将样本1分到类别2中。因此虽然这个输出层中使用了一个非线性的激活函数，但本质上还是一个线性模型。

为了突破这个限制，我们可以在网络中加入一个或多个隐藏层，使其能够处理更普遍的函数关系。最简单的办法就是将许多个全连接层堆在一起，并将本层的输出作为下一层的输入，直至输出层。

【注】：全连接层是指某一层中的每一个神经元都与上一层中的所有神经元相连接（输入层除外），这样的层就叫做全连接层。

下面我们讨论的多层感知机只讨论全为全连接层的情况，即全连接网络。四个输入节点、五个隐藏节点、三个输出节点的两层全连接网络可如下表示：

![](/figure/14.png)

以输出o_1为例，可以写出其表达式：

$$
y_1 = f_1^{[2]}(z_1^{[2]})
$$

$$
z_1^{[2]} = h_1w_{11}^{[2]} + h_2w_{21}^{[2]} + h_3w_{31}^{[2]} + h_4 w_{41}^{[2]}+ w_{51}^{[2]}h_5 + b_1^{[2]}
$$

$$
h_1 = f_1^{[1]}(z_1^{[1]})
$$

$$
h_2 =f_2^{[1]}(z_2^{[1]})
$$

$$
h_3 =f_3^{[1]}(z_3^{[1]})
$$

$$
h_4 = f_4^{[1]}(z_4^{[1]})
$$

$$
h_5 = f_5^{[1]}(z_5^{[1]})
$$

$$
z_1^{[1]} = x_1w_{11}^{[1]} + x_2w_{21}^{[1]} + x_3w_{31}^{[1]} + x_4w_{41}^{[1]}+ b_1^{[1]}
$$

$$
z_2^{[1]} = x_1w_{12}^{[1]} + x_2w_{22}^{[1]} + x_3w_{32}^{[1]} + x_4w_{42}^{[1]}+ b_1^{[1]}
$$

$$
z_3^{[1]} = x_1w_{13}^{[1]} + x_2w_{23}^{[1]} + x_3w_{33}^{[1]} + x_4w_{43}^{[1]}+ b_1^{[1]}
$$

$$
z_4^{[1]} = x_1w_{14}^{[1]} + x_2w_{24}^{[1]} + x_3w_{34}^{[1]} + x_4w_{44}^{[1]}+ b_1^{[1]}
$$

$$
z_5^{[1]} = x_1w_{15}^{[1]} + x_2w_{25}^{[1]} + x_3w_{35}^{[1]} + x_4w_{45}^{[1]}+ b_1^{[1]}
$$

写成紧密形式：

$$
\mathbf{\hat{Y}} = \mathbf{F}^{[2]}(\mathbf{Z}^{[2]})
$$

$$
\mathbf{Z}^{[2]} = \mathbf{H} \mathbf{W}^{[2]}  + \mathbf{b}^{[2]}
$$

$$
\mathbf{H} = \mathbf{F}^{[1]}(\mathbf{Z}^{[1]})
$$

$$
\mathbf{Z}^{[1]} = \mathbf{X} \mathbf{W}^{[1]} + \mathbf{b}^{[1]}\\
$$

【注】这里的[1]和[2]分别表示第一层和第二层，**F**( · )表示激活函数。

### 常见的激活函数

#### ReLU函数

修正线性单元（rectified linear unit, ReLU），ReLU是目前最受欢迎的激活函数，因为其不会像sigmoid函数一样出现“梯度消失“（稍后讨论），同时也能产生很好的效果。其定义为：对于给定元素x，ReLU函数返回的是该元素x与0中的最大值，即

$$
ReLU(x) = \max(x,0)
$$

通俗地说就是当x小于零的时候函数返回0，当x大于零的时候返回x。

```python
import torch
from d2l import torch as d2l

# 1.ReLU函数
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x) # 输入为负时，函数的导数值为0；输入为正时，函数的导数值为1
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

![](/figure/15.png)

ReLU(x)有很多变体，包括参数化ReLU(pReLU): 

$$
pReLU(x) = \max (x, 0) + \alpha \min (0, x)
$$

#### sigmoid函数

对于任意一个实数域的输入，可以将其压缩到范围为0到1的输出。

$$
sigmoid(x) = \frac{1}{1+e^{-x}}
$$

```python
# 2.sigmoid函数
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

![](/figure/16.png)

#### tanh函数（双曲正切）

与sigmoid函数类似，tanh函数也能将任意实数域上的输入压缩到-1到1的范围输出：

$$
sigmoid(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
$$

```python
# 3.tanh函数（双曲正切）
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

![](/figure/17.png)

### 损失函数与随机梯度下降

假设一个有L层的网络，输入为第0层，共有d个输入，输出为第L层，共有q个输出，第l个隐藏层共有h_l个神经元，则：

$$
输出层:\hat{y_i} = a_i^{[L]} = a_i(输出层可以省略上标)
$$

$$
输入层:x_i = a_i^{[0]}
$$

$$
第l层的第i个输出:a_i^{[l]} =f_i^{[l]}(z_i^{[l]})
$$

$$
损失函数与所有层的权重和偏置相关:J(\mathbf{W}, \mathbf{b})
$$

将每一层的权重和偏置合并为一个参数W

![](/figure/matrix/1.png)

由每个向量右下角的维度可以看出，每一层隐藏层计算时，都是**手动**在下一层的输入最前面加上元素“1”，例如：从输入层（第0层）到第1层时，是一个1×(1+d)维度的向量与一个维度为(1+d)×h_1的矩阵W相乘，得出的线性输出Z^[1]应该是h_1维的，然后再对其使用激活函数（不会影响向量维数）；但是第1层传入到第2层时，就变成了(1+h_2)维，即在传入下一层之前，会在本层的输出向量最前面加一个元素"1"，以便于下一层计算偏置。

【注】这里的“1”只是在推导数学公式时为了便于理解和推导而手动添加的，在代码中不用考虑每一层都添加“1”。

为了方便使用梯度下降法，将所有层的参数矩阵W都放入一个列向量Θ中：

![](/figure/matrix/2.png)

将损失函数设为均方误差，即：

![](/figure/matrix/3.png)

BP算法的基本思想：
①设置初始系数Θ为较小的随机非零值；
②给定输入/输出样本对，计算网络输出，完成前向传播；
③计算目标函数 J 。如 J<ε ，则训练成功，退出；否则转入④；
④反向传播计算由输出层，按照梯度下降法将误差反向传播，逐层调整权值，转入②。

设置初始系数值为Θ0，k时刻的系数值为Θk，使用泰勒级数展开，则有：

$$
J(\Theta_{k+1}) = J(\Theta_k) + (\frac{dJ}{d\Theta})^T\Delta \Theta_k+...
$$

$$
忽略高阶项:J(\Theta_{k+1}) = J(\Theta_k) + (\frac{dJ}{d\Theta})^T\Delta \Theta_k
$$

需要选择合适的ΔΘk使得损失函数 J 最小。

$$
最直接的办法就是令\Delta \Theta_k = - \alpha \frac{dJ}{d\Theta}
$$

$$
此时J(\Theta_{k+1}) = J(\Theta_k) - \alpha (\frac{dJ}{d\Theta})^2
$$

这样都可以保证每一次训练之后所得到的损失函数都会比训练之前的损失函数更小，从而收敛到最小。

$$
J(\Theta) = \frac{1}{2} \sum_{j=1}^q (y_j - \hat{y_j})^2 = \frac{1}{2} \sum_{j=1}^q(e_j)^2
$$

此时：

![](/figure/matrix/4.png)

下面使用sigmoid作为激活函数进行讨论。

1. 先讨论权重的情况：

$$
\frac{\partial J}{\partial w_{ij}^{[l]}}=\frac{\partial J}{\partial e_i} \frac{\partial e_i}{\partial w_{ij}^{[l]}}
$$

​	当l=L时：

$$
\frac{\partial J}{\partial w_{ij}^{[L]}}=\frac{\partial J}{\partial e_i} \frac{\partial e_i}{\partial h_i^{[L]}} \frac{\partial h_i^{[L]}}{\partial z_i^{[L]}} \frac{\partial z_i^{[L]}}{\partial w_{ij}^{[L]}}
$$

$$
因为J = \frac{1}{2}\sum_{i=1}^q(e_i)^2
$$

$$
\frac{\partial J}{\partial e_i} = e_i
$$

$$
因为e_i=y_i-h_i^{[L]}
$$

$$
\frac{\partial e_i}{\partial h_i^{[L]}} = -1
$$

$$
因为h_i = f_i(z_i),这里所有的f(·)都是sigmoid函数
$$

$$
\frac{d(sigmoid(x))}{dx}=sigmoid(x)(1-sigmoid(x))
$$

$$
\frac{\partial h_i^{[L]}}{\partial z_i^{[L]}} = h_i^{[L]}(1-h_i^{[L]})
$$

$$
z_i^{[L]} = \sum_{j=0}^{h_{L-1}} w_{ij}^{[L]}h_j^{[L-1]}
$$

$$
\frac{\partial z_i^{[L]}}{\partial w_{ij}^{[L]}} = h_j^{[L-1]}
$$

​	综上

$$
\frac{\partial J}{\partial w_{ij}^{[L]}} = -e_i h_i^{[L]}(1-h_i^{[L]})h_j^{[L-1]}
$$

$$
令\delta_i^{[L]}=e_i h_i^{[L]}(1-h_i^{[L]}),则\Delta w_{ij}^{[L]}(k)=\alpha \delta_i^{[L]}h_j^{[L-1]}
$$

​	当l < L时：

$$
\frac{\partial J}{\partial w_{ij}^{[l]}}=\frac{\partial J}{\partial e_i} \frac{\partial e_i}{\partial h_i^{[L]}} \frac{\partial h_i^{[L]}}{\partial z_i^{[L]}} \frac{\partial z_i^{[L]}}{\partial h_i^{[L-1]}}\frac{\partial h_i^{[L-1]}}{\partial z_i^{[L-2]}}...\frac{\partial h_i^{[l+1]}}{\partial z_i^{[l]}}\frac{\partial z_i^{[l]}}{\partial w_{ij}^{[l]}}
$$

$$
\frac{\partial z_i^{[l]}}{\partial w_{ij}^{[l]}} = h_j^{[l-1]}
$$

$$
\frac{\partial J}{\partial w_{ij}^{[l]}}=-(\sum_{j=1}^m w_{ji}^{[l+1]}\delta_{j}^{[l+1]})(h_i^{[l]})^T h_j^{[l-1]}
$$

​	经过整理后可得：

$$
令\delta_i^{[l]}=(\sum_{j=1}^m w_{ji}^{[l+1]}\delta_{j}^{[l+1]})(h_i^{[l]})^T ,则\Delta w_{ij}^{[l]} = \alpha \delta_i^{[l]}h_j^{[l-1]}
$$

​	总结来说，对于权重系数，均可表示为：

$$
\Delta w_{ij}^{[l]} = \alpha \delta_i^{[l]}h_j^{[l-1]}
$$

$$
\frac{\partial J}{\partial z_i^{[l]}}= -\delta_i^{[l]}
$$

$$
当l=L时:\delta_i^{[L]}=e_i h_i^{[L]}(1-h_i^{[L]})
$$

当l<L时:

$$
\delta_i^{[l]}=(\sum_{j=1}^m w_{ji}^{[l+1]}\delta_{j}^{[l+1]})(h_i^{[l]})^T
$$

2. 然后来讨论偏置

​	已知：

$$
z_i^{[l]}=\sum_j w_{ij}^{[l]}h_j^{[l-1]} + b_i^{[l]}
$$

​	因此

$$
\frac{\partial J}{\partial b_i^{[l]}} = \frac{\partial J}{\partial z_i^{[l]}}\frac{\partial z_i^{[l]}}{\partial b_i^{[l]}}且\frac{\partial J}{\partial z_i^{[l]}}= -\delta_i^{[l]},\frac{\partial z_i^{[l]}}{\partial b_i^{[l]}}=1
$$

$$
\frac{\partial J}{\partial b_i^{[l]}} =\delta_i^{[l]}·1=-\delta_i^{[l]}
$$

$$
\Delta b_i^{[l]}=-\alpha\frac{\partial J}{\partial b_i^{[l]}}=\alpha \delta_i^{[l]}
$$

3. 参数更新：在整个神经网络中，系数的更新是从输出层向输入层更新，即从L层、L-1层、...、1层的顺序、使用梯度下降法进行更新：
   即当 l > 0 时

$$
w_{ij}^{[l]} \leftarrow w_{ij}^{[l]}-\Delta w_{ij}^{[l]} = w_{ij}^{[l]}-\alpha \delta_i^{[l]}h_j^{[l-1]}
$$

$$
b_{i}^{[l]} \leftarrow b_{i}^{[l]}-\Delta b_{i}^{[l]} = b_{i}^{[l]}-\alpha \delta_i^{[l]}
$$

​	写成矩阵形式：

![](/figure/matrix/5.png)

​	以矩阵形式更新：

$$
\Theta \leftarrow \Theta-\alpha\frac{dJ}{d\Theta}
$$


### 多层感知机的实现

#### 多层感知机从零实现

了解了多层感知机参数的更新方式，我们就来进行代码学习。编写代码时不需要像上面的公式一样一次一次来求解微分，pytorch中已经封装好了多种优化方法可以使用。

这里我们将上期所使用到的train_ch3模块封装到了一个文件Train.py中便于使用，Train.py中的代码如下：

```python
import torch
from IPython import display
from d2l import torch as d2l

def accuracy(y_hat, y):
    '''计算预测正确的数量'''
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())

def evaluate_accuracy(net, data_iter):
    '''计算在指定数据集上模型的精度'''
    if isinstance(net, torch.nn.Module):
        net.eval() # 将模式设置为评估模式
    matric = Accumulator(2) # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            matric.add(accuracy(net(X), y), y.numel())
    return matric[0] / matric[1]

class Accumulator:
    def __init__(self, n):
        self.data = [0, 0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0, 0] * len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]
    
def train_epoch_ch3(net, train_iter, loss, updater): #@save
    """训练模型一个迭代周期"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
            # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
    # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
        # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
        # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]

class Animator:
    '''在动画中绘制数据'''
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)):
        # 增量地画出多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols ==1:
            self.axes = [self.axes, ]
        # 实用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend
        )
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)        

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save
    """训练模型"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```
下面还是使用Fashion-MNIST中的图像数据进行训练，创建一个含有一层隐藏层的神经网络。

```python
import torch 
from torch import nn
from d2l import torch as d2l
from Train import train_ch3
```

```python
batch_size = 256 # 每次取出256个样本来进行训练
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

Fashion-MNIST中的图像数据是28×28=784个灰度像素，隐藏层中添加256个神经元。

```python
num_inputs = 784
num_outputs = 10
num_hiddens = 256 # 隐藏层神经元个数

# 参数初始化
W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
```

手动定义一个ReLU函数、神经网络、损失函数（交叉熵损失）

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)

def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X @ W1 + b1)  # 将激活函数作用与第一层的线性输出
    return (H @ W2 + b2)

loss = nn.CrossEntropyLoss(reduction='none') # 使用交叉熵损失函数
```

定义训练周期、学习率进行训练

```python
num_epochs = 10
lr = 0.1

updater = torch.optim.SGD(params, lr=lr)
train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

![](/figure/19.png)

#### 多层感知机的简洁实现

上面我们使用def net来手动定义了一个神经网络，但是pytorch中也可以添加多层的神经网络。

```python
import torch
from torch import nn
from d2l import torch as d2l
from Train import train_ch3
```

```python
# 线性堆叠，先进行平展，第一层线性层是784输入256输出，然后使用激活函数进行处理，第二层线性层是256输入10输出
net =nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) 

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01) # 使用正态分布初始化权重

net.apply(init_weights)
```

```python
batch_size = 256
lr = 0.1
num_epochs = 10
loss = nn.CrossEntropyLoss(reduction='none') # 使用交叉熵损失函数

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr) # 使用随机梯度下降优化器
train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) # 训练模型
```

![](/figure/20.png)



## 性能优化

模型的性能与多种因素相关，最直接的就是训练模型使用的数据集、模型参数以及模型复杂度。

### 模型的初始化与数据集分割

在初始化模型时，为了能够顺利进行后续的梯度下降，我们需要让模型的初始权值不全为零（否则输出始终是恒定值，无法求梯度），常见的初始化方法有：

1. 最简单的方法：把所有权值在[-1,1]区间内按均值或高斯分布进行初始化；
2. 把所有权值按照均值为μ，标准差为σ的正态分布来进行初始化；
3. Xavier初始化：为了使网络中的信息更好地流动，每一层输出的方差应该尽量相等。因此需要实现下面的均匀分布：

$$
W \sim U \begin{bmatrix} -\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}} \end{bmatrix}
$$

在数据集进行分割时，需要包括：训练数据、验证数据、测试数据，通常三者的比例为70:15:15或6:2:2，当数据集很多时，训练集和验证集的数据可适当减少。

常见的训练与测试的方法：K折交叉验证——原始数据被分成K个不重叠的子集，每次使用K-1个数据集进行训练，剩下一个子集来进行验证，一共重复K次，每次都使用与之前不同的验证集，所得到的实验结果取平均来估计训练和验证误差。

### 欠拟合和过拟合

欠拟合和过拟合是在训练过程中经常出现的现象。
欠拟合是指：在模型训练的过程中误差始终较大，没有达到训练的效果，可能是由于模型过于简单，无法捕获试图学习的模式；
过拟合是指：模型训练得过度，使得模型对于训练集的表现较好，但在测试集中的表现较差，即泛化能力较差。

针对过拟合，有常见的几种方法：

1. 权重衰减法

   权重衰减法就是在损失函数中增加一个惩罚项，当模型中的参数训练得过大且对损失模型的优化没那么大时，就停止训练，常见的权重衰减法为L2正则化，在L2正则化下新的损失函数为：
   
$$
J(\mathbf{w})+\frac{\lambda}{2} \| \mathbf{w} \| ^2
$$
   
   惩罚项约束了不能过大，在梯度下降时：
   
$$
\frac{J(\mathbf{w})}{d\mathbf{w}}+\lambda \mathbf{w}
$$

3. 暂退法（dropout）

   在模型训练的过程中，损失函数可能对个别参数特别敏感，这些参数的持续变换就能让损失函数快速变小，使得其他参数得不到很好的训练，在遇到一些对这些参数不太敏感的数据时，模型的表现就会很差。这个时候就会使用暂退法，每次训练的时候都随机将其中一部分节点置零，只使用其他未置零的参数进行训练。

   ![](/figure/21.png)

   关于权重衰减法和暂退法，若还有疑惑，可以点击网址：[调教神经网络咋这么难？【白话DeepSeek03】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1RqXRYDEe2?spm_id_from=333.788.videopod.sections&vd_source=a9e7307fd7cf6202b163a39b03746807)

4. 动量法

   在使用少样本随机梯度下降法对模型进行训练时，可能会遇到一些病态曲率（后续会详细介绍）。这个时候使用简单的少样本随机梯度下降法就不能很好地训练模型，因此需要使用动量法来更新参数：

$$
v_t = \alpha v_{t-1} - \epsilon g_t
$$

$$
\Delta \theta \leftarrow v_t
$$

$$
\theta_{t+1} \leftarrow \theta_t + \Delta \theta
$$

$$
其中:\epsilon是学习率,\alpha是动量参数,当没有达到训练停止的要求时:
$$

$$
在训练集中的m个样本\{ x^{(1)}...x^{(m)} \}的小批量，对应目标为y^{(i)}

$$
梯度估计g\leftarrow \frac{1}{m} \nabla_{\theta}\sum_i L(f(x^{(i)};\theta),y^{(i)})
$$

动量法可视化以及详细原理可见《动手学深度学习》11.6节或点击网址：[Why Momentum Really Works](https://distill.pub/2017/momentum/)

4. 自适应梯度算法

在梯度下降法中，有一个至关重要的参数——学习率，当前训练效果与最优点越远，应该采用越大的学习率；当前寻来你效果与最优点越近，就应该采用越小的学习率。用数学语言描述就是：具有较大偏导数的参数应该使用一个较大的学习率，具有较小偏导数的参数则应该使用一个较小的学习率。

但这种方法也存在缺陷：学习率是单调递减的，训练后期学习率过小可能会导致训练困难或提前结束，因此需要设置一个全局的初始学习率。


